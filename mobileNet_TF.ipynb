{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using pretrained MobileNetV2\n",
    "\n",
    "Here we want to use a pre-trained mobile net and train it on the hand images from before. If we use the model already implemented in keras, we can use the weights from ImageNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "#from tensorflow.keras.applications import imagenet_utils\n",
    "from tensorflow.keras.layers import Dense,GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications.mobilenet import MobileNet\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
    "\n",
    "from PIL import Image # used for loading images\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
    "import numpy as np\n",
    "#from IPython.display import Image\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'tensorflow._api.v2.version' from 'C:\\\\Users\\\\werth\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\envs\\\\workshop\\\\lib\\\\site-packages\\\\tensorflow\\\\_api\\\\v2\\\\version\\\\__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "print(tf.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The model\n",
    "## Loading the mobileNet model without top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\werth\\AppData\\Local\\Continuum\\anaconda3\\envs\\workshop\\lib\\site-packages\\keras_applications\\mobilenet_v2.py:295: UserWarning: MobileNet shape is undefined. Weights for input shape(224, 224) will be loaded.\n",
      "  warnings.warn('MobileNet shape is undefined.'\n"
     ]
    }
   ],
   "source": [
    "base_model=MobileNetV2(weights='imagenet',include_top=False) #imports the mobilenet model and discards the last 1000 neuron layer.\n",
    "\n",
    "x=base_model.output\n",
    "x=GlobalAveragePooling2D()(x)\n",
    "x=Dense(1024,activation='relu')(x) #we add dense layers so that the model can learn more complex functions and classify for better results.\n",
    "x=Dense(1024,activation='relu')(x) #dense layer 2\n",
    "x=Dense(512,activation='relu')(x) #dense layer 3\n",
    "\n",
    "preds=Dense(2,activation='softmax')(x) #final layer with softmax activation\n",
    "\n",
    "#specify the inputs\n",
    "#specify the outputs\n",
    "model=Model(inputs=base_model.input,outputs=preds)\n",
    "#now a model has been created based on our architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualize models to compare with and without top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,layer in enumerate(model.layers):\n",
    "   # print(i,layer.name)\n",
    "    1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for comparison, the original mobileNetV2 below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_model=MobileNetV2(weights='imagenet',include_top=True) #imports the mobilenet model and discards the last 1000 neuron layer.\n",
    "for i,layer in enumerate(compare_model.layers):\n",
    "    #print(i,layer.name)\n",
    "    1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What happens when removing the last dense layers (on top)?\n",
    "\n",
    "the weights in a convolutional layer are fixed-size. They are the size of the kernel x filters. Example: a 3x3 kernel of 10 filters. A convolutional layer doesn't care about the size of the input image. It just does the convolutions and present a resulting image based on the size of the input image. (Search for some illustrated tutorials about convolutions if this is unclear)\n",
    "\n",
    "now the weights in a dense layer are totally dependent on the input size. It's one weight per element of the input. So this demands that your input be always the same size, or else you won't have proper learned weights.\n",
    "\n",
    "Because of this, removing the final dense layers allows you to define the input size (see in documentation). (And the output size will increase/decrease accordingly)\n",
    "\n",
    "Thanks to great stackoverflower [Daniel MÃ¶ller](https://stackoverflow.com/users/2097240/daniel-m%c3%b6ller)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing the layers to not be trainable\n",
    "\n",
    "The model is set up like a numpy array where you can reach each layer by itself. You can set now all layers of the base model frozen.\n",
    "important is that after setting layer trainable =False, the model has to `be compile()` to take effect.\n",
    "\n",
    "\n",
    "See Keras [webside](https://keras.io/getting-started/faq/#how-can-i-freeze-keras-layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=100 #149 are all layers before dense output\n",
    "for layer in model.layers[:n]:\n",
    "    layer.trainable=False\n",
    "for layer in model.layers[n:]:\n",
    "    layer.trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['categorical_accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For optimal throughput I used a double-buffering approach where the next request is already being prepared (by the CPU) while the current one is still being processed (by the GPU). This way the CPU and GPU are never waiting for one another.\n",
    "\n",
    "(Fun fact: for V2 it was actually worth doing triple buffering but for V1 that made no difference in speed. This shows that V2 is much more efficient.)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model\n",
    "## Loading the data with data generator\n",
    "\n",
    "Now we use the generator function again to load the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 264 images belonging to 2 classes.\n",
      "Found 10 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen =ImageDataGenerator()\n",
    "\n",
    "train_batch_generator =datagen.flow_from_directory(directory=\"C:/Users/werth/Pictures/small_structured/input/Train/\",\n",
    "                                     classes=['Faust', 'Offen'],\n",
    "                                     target_size=[170,255],\n",
    "                                     class_mode='categorical',\n",
    "                                     batch_size=8,\n",
    "                                     color_mode='rgb',\n",
    "                                     shuffle=True,\n",
    "                                     seed=42)\n",
    "\n",
    "valid_batch_generator =datagen.flow_from_directory(directory=\"C:/Users/werth/Pictures/small_structured/input/Test/\",\n",
    "                                     classes=['Faust', 'Offen'],\n",
    "                                     target_size=[170,255],\n",
    "                                     class_mode='categorical',\n",
    "                                     batch_size=5,\n",
    "                                     color_mode='rgb',\n",
    "                                     shuffle=True,\n",
    "                                     seed=42)\n",
    "\n",
    "#img_batch = next(batches)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "33/33 [==============================] - 97s 3s/step - loss: 0.3697 - categorical_accuracy: 0.8447 - val_loss: 11.7013 - val_categorical_accuracy: 0.3000\n",
      "Epoch 2/5\n",
      "33/33 [==============================] - 34s 1s/step - loss: 0.2920 - categorical_accuracy: 0.9394 - val_loss: 3.5547 - val_categorical_accuracy: 0.3000\n",
      "Epoch 3/5\n",
      "33/33 [==============================] - 30s 904ms/step - loss: 0.1748 - categorical_accuracy: 0.9545 - val_loss: 17.3694 - val_categorical_accuracy: 0.3000\n",
      "Epoch 4/5\n",
      "33/33 [==============================] - 28s 852ms/step - loss: 0.3986 - categorical_accuracy: 0.9318 - val_loss: 23.5045 - val_categorical_accuracy: 0.3000\n",
      "Epoch 5/5\n",
      "33/33 [==============================] - 28s 851ms/step - loss: 0.0212 - categorical_accuracy: 0.9924 - val_loss: 9.9980 - val_categorical_accuracy: 0.4000\n",
      "Wall time: 3min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "step_size_train=train_batch_generator.n//train_batch_generator.batch_size\n",
    "step_size_valid=valid_batch_generator.n//valid_batch_generator.batch_size\n",
    "\n",
    "history=model.fit_generator(generator=train_batch_generator,\n",
    "                    steps_per_epoch=step_size_train,\n",
    "                    validation_data=valid_batch_generator,\n",
    "                    epochs=5,\n",
    "                    callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the model \n",
    "\n",
    "## Tensorboard\n",
    "\n",
    "before training the model we activate tensorboard. Tensorboard can monitor the training and validation of the model and visualize the process. Thereby we can identify where optimization is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime, os\n",
    "%load_ext tensorboard.notebook\n",
    "\n",
    "logs_base_dir = \".\\logs\"#create folder for \n",
    "os.makedirs(logs_base_dir, exist_ok=True)\n",
    "%tensorboard --logdir {logs_base_dir}\n",
    "\n",
    "logdir = os.path.join(logs_base_dir, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively you can also use a file writer and save the information in the folder. Than you can call the tensorboard via the console and open tensorboard in another tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Timed out waiting for TensorBoard to start. It may still be running as pid 12504."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "writer = tf.summary.FileWriter( \"./logs\", tf.get_default_graph())\n",
    "%tensorboard --logdir=.\\logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simply printing the history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history.history['categorical_accuracy'])\n",
    "plt.plot(history.history['val_categorical_accuracy'])\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
